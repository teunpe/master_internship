{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from functorch import grad, vmap, jacrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='pipeline.log', encoding='utf-8', level=logging.DEBUG, filemode='w')\n",
    "\n",
    "logger.info('Defining models')\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "            )\n",
    "        self.start_layer = 0\n",
    "        self.end_layer = len(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x)\n",
    "        if self.start_layer == 0:\n",
    "            x = x.view(-1, 28*28)\n",
    "        for i in range(self.start_layer, self.end_layer):\n",
    "            x = self.model[i](x)\n",
    "        if self.end_layer == len(self.model):\n",
    "            x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def set_start_layer(self, i):\n",
    "        self.start_layer = i\n",
    "\n",
    "    def set_end_layer(self, i):\n",
    "        self.end_layer = i\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1,32,(3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(32, 64, (3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1600, 10)\n",
    "        )\n",
    "        self.start_layer = 0\n",
    "        self.end_layer = len(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.start_layer, self.end_layer):\n",
    "            x = self.model[i](x)\n",
    "        if self.end_layer == len(self.model):\n",
    "            x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def set_start_layer(self, i):\n",
    "        self.start_layer = i\n",
    "\n",
    "    def set_end_layer(self, i):\n",
    "        self.end_layer = i\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "logger.info('Initializing settings')\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=2, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                    help='disables macOS GPU training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "def train_model(modeltype: str):\n",
    "    logger.info(f'Training {modeltype} model')\n",
    "    match modeltype:\n",
    "        case 'cnn':\n",
    "            model = ConvNet().to(device)\n",
    "        case 'fc':\n",
    "            model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), f\"mnist_{modeltype}.pt\")\n",
    "\n",
    "def load_model(filename: str, type: str='cnn', test: bool=False):\n",
    "    \"\"\" \n",
    "    load the model from the .pt file and evaluate on the test set if requested\n",
    "    :param filename: str of model save filename\n",
    "    :param test: bool dictating if the model is tested\n",
    "    :return: loaded model\n",
    "    \"\"\"\n",
    "    logger.info(f'Loading model from file {filename}')\n",
    "    modeldict = torch.load(filename)\n",
    "    if type == 'cnn':\n",
    "        model = ConvNet()\n",
    "    else:\n",
    "        model = Net()\n",
    "    model.load_state_dict(modeldict)\n",
    "    if test:\n",
    "        test(model, device, test_loader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('mnist_fc.pt', 'fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_axis(tensor: torch.Tensor): \n",
    "    \"\"\"\n",
    "    Set a random value in an input-shaped tensor of zeros to 1, dictating a random direction along the axis to take the steps in.\n",
    "    Sets one direction that is used for each sample in the batch.\n",
    "    :param tensor: input-shaped tensor of zeros\n",
    "    :return: tensor of zeros and a 1\n",
    "    \"\"\"\n",
    "    # take first entry in batch\n",
    "    sampletensor = tensor[0]\n",
    "    input_length = len(sampletensor.flatten())\n",
    "\n",
    "    random_numbers = np.array(random.sample(range(0, input_length), input_length))\n",
    "    random_numbers = random_numbers.reshape(np.array(sampletensor).shape)\n",
    "\n",
    "    random_index = random.sample(range(0, input_length), 1)\n",
    "    random_index = np.where(random_numbers == random_index)\n",
    "\n",
    "    zerotensor = torch.zeros_like(sampletensor)\n",
    "    zerotensor[random_index] = 1\n",
    "    dims = (tensor.shape[0],)\n",
    "    for d in range(len(tensor.shape)-1):\n",
    "        dims += (1,)\n",
    "    return zerotensor.repeat(dims)\n",
    "\n",
    "def direction(method: str, input: torch.Tensor, grad: torch.Tensor=None, modeltype=Net):\n",
    "    \"\"\"\n",
    "    calculate the direction of a step given the requested direction type and the input\n",
    "    :param method: string defining the method for calculating the direction ('axis', 'random' or 'gradient')\n",
    "    :param input: input to the layer to be investigated\n",
    "    :return: tensor array with the same shape as the input describing the direction\n",
    "    \"\"\"\n",
    "    dir = torch.ones_like(input)\n",
    "    match method:\n",
    "        case 'axis':\n",
    "            dir = torch.zeros_like(dir)\n",
    "            dir = random_axis(dir)\n",
    "        case 'random':\n",
    "            dir = torch.rand_like(dir) #normal\n",
    "        case 'gradient':\n",
    "            if modeltype == ConvNet:\n",
    "                ones = np.ones(grad.T.shape[3:])\n",
    "                grad = grad.T.flatten(start_dim=3)\n",
    "                # print(grad.shape, ones.shape)\n",
    "                dir = np.dot(grad, ones.flatten()).T\n",
    "            elif modeltype == Net:\n",
    "                grad = grad.squeeze()\n",
    "                dir = grad.transpose(1,2)@torch.ones(grad.shape[1:2])\n",
    "                dir = dir.detach()\n",
    "        case _:\n",
    "            warnings.warn('Unknown direction type, using all directions', RuntimeWarning)\n",
    "            pass\n",
    "    if type(dir) != torch.Tensor:\n",
    "        dir = torch.tensor(dir)\n",
    "    # print(dir.shape)\n",
    "    return dir/torch.sum(dir) #normalisatie?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_runs(startinput: torch.Tensor, model, group: list, dirtype: str=None, steps: list=np.logspace(-5,1,6)):\n",
    "    \"\"\"\n",
    "    run through the model while creating inputs at the different steps with the given direction. save the outputs for further inspection\n",
    "    :param input: tensor input to the first layer\n",
    "    :param model: neural network \n",
    "    :param dirtype: string describing the method used to find a direction\n",
    "    :return: inputs into each layer with step adjustments, corresponding outputs, dirs, steps and grads\n",
    "    \"\"\"\n",
    "    startinput.requires_grad = True\n",
    "    logger.info(f'Starting forward runs with group {group} and steps {steps}')\n",
    "\n",
    "    #for dicts:\n",
    "    dictsteps = []\n",
    "    dictinputs = []\n",
    "    dictoutputs = []\n",
    "    dictdirs = []\n",
    "    dictgrads = []\n",
    "\n",
    "    logger.info(f'{group}')\n",
    "    input = startinput\n",
    "    firstlayer = model.model[group[0]]\n",
    "\n",
    "    firstlayer_idx = np.where(np.isin(model.model, firstlayer))[0][0]\n",
    "    if firstlayer_idx > 0:\n",
    "        model.set_start_layer(0)\n",
    "        model.set_end_layer(firstlayer_idx)\n",
    "        input = model(input)\n",
    "    elif type(model) != ConvNet:\n",
    "        input = input.flatten(start_dim=1)\n",
    "    \n",
    "    model.set_start_layer(group[0])\n",
    "    model.set_end_layer(group[-1])\n",
    "\n",
    "    print(input.shape)\n",
    "    # gradient = torch.autograd.functional.jacobian(model, input)\n",
    "    # we use vmap to compute the jacobian for each batch\n",
    "    grads = []\n",
    "    for i in range(input.shape[0]):\n",
    "        print(input[i].shape)\n",
    "        gradient = torch.autograd.functional.jacobian(model, input)\n",
    "        grads.append(gradient)\n",
    "    grads = torch.tensor(grads)\n",
    "    dir = direction(dirtype, input, gradient, modeltype=type(model))\n",
    "\n",
    "    dictsteps.append(0.)\n",
    "    dictinputs.append(input.detach().clone())\n",
    "    dictoutputs.append(model(input).detach().clone())\n",
    "    dictdirs.append(dir)\n",
    "    dictgrads.append(grads)\n",
    "\n",
    "    for step in steps:\n",
    "        logger.info(f'{step}')\n",
    "        # print(input.shape, dir.shape)\n",
    "        newinput = input.detach() + step*dir.detach()\n",
    "        newoutput = model(newinput)\n",
    "        # print(newoutput.shape, grad.shape, dir.shape)\n",
    "        inp = newinput.detach().clone()\n",
    "        outp = newoutput.detach().clone()\n",
    "\n",
    "        grads = []\n",
    "        for i in range(input.shape[0]):\n",
    "            dictgrad = torch.autograd.functional.jacobian(model, input)\n",
    "            grads.append(dictgrad)\n",
    "        dictgrad = torch.tensor(grads)\n",
    "        logger.info(f'{inp.shape, outp.shape, dir.shape, dictgrad.shape}')\n",
    "        dictsteps.append(step)\n",
    "        dictinputs.append(inp)\n",
    "        dictoutputs.append(outp)\n",
    "        dictdirs.append(dir)\n",
    "        dictgrads.append(dictgrad)\n",
    "\n",
    "    dic = {'step': dictsteps, 'input': dictinputs, 'output': dictoutputs, 'grad': dictgrads, 'dir': dictdirs}\n",
    "    df = pd.DataFrame.from_dict(dic)\n",
    "    model.set_start_layer(0)\n",
    "    model.set_end_layer(len(model.model))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40992/3427267139.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(next(iter(test_loader))[0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mforward_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 42\u001b[0m, in \u001b[0;36mforward_runs\u001b[0;34m(startinput, model, group, dirtype, steps)\u001b[0m\n\u001b[1;32m     40\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(model, \u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     41\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(gradient)\n\u001b[0;32m---> 42\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m direction(dirtype, \u001b[38;5;28minput\u001b[39m, gradient, modeltype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(model))\n\u001b[1;32m     45\u001b[0m dictsteps\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0.\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "input = torch.tensor(next(iter(test_loader))[0])\n",
    "df = forward_runs(input, model, [1,5], 'random', [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_exp_act(df: pd.DataFrame, conv: bool=False):\n",
    "    \"\"\" \n",
    "    calculate the expected outputs assuming linearity, and take the actual outputs from the model\n",
    "    :param df: \n",
    "    :return: expected outputs assuming linearity and actual outputs\n",
    "    \"\"\"\n",
    "    logger.info('Calculating non-linearities')\n",
    "    dictgroups = []\n",
    "    dictsteps = []\n",
    "    dictlins = []\n",
    "    for group in np.unique(df['group']):\n",
    "        for step in np.unique(df['step']):\n",
    "            print(group, step)\n",
    "            logger.info(f'{group} {step}')\n",
    "            dfoi = df[(df['group']==str(group)) & (df['step']==step)]\n",
    "\n",
    "            step = list(dfoi['step'])[0]\n",
    "            act = list(dfoi['output'])[0]\n",
    "            grad = list(dfoi['grad'])[0]\n",
    "            dir = list(dfoi['dir'])[0]\n",
    "\n",
    "            nostep = list(df[(df['group'] == str(group)) & (df['step'] == 0.)]['output'])[0]\n",
    "            if conv:\n",
    "                grad = grad.flatten(start_dim=len(grad.shape)-3)\n",
    "                dir = dir.flatten()\n",
    "            print(nostep.shape, grad.shape, dir.shape)\n",
    "            exp = nostep + step*np.matmul(grad, dir)\n",
    "            diff = exp - act\n",
    "            lin = np.linalg.norm(diff)/step\n",
    "            logger.info(f'{grad.shape}, {dir.shape}, {nostep.shape}, {exp.shape}')\n",
    "\n",
    "            dictgroups.append(group)\n",
    "            dictsteps.append(step)\n",
    "            dictlins.append(lin)\n",
    "\n",
    "    dic = {'group': dictgroups, 'step': dictsteps, 'lin': dictlins}\n",
    "    df = pd.DataFrame.from_dict(dic)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model_type, df: pd.DataFrame, dir: str, ylog: bool):\n",
    "    \"\"\" \n",
    "    plot the L2 norms per step size for each layer in the model\n",
    "    :param model: model to be investigated\n",
    "    :param lindf: \n",
    "    :param dir: str describing step direction\n",
    "    \"\"\"\n",
    "    logger.info(f'plotting for model {model_type} and dir {dir}')\n",
    "    zero = np.zeros(len(df['step'].unique()))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, layout='constrained', figsize=(7,4))\n",
    "    groups = np.unique(df['group'])\n",
    "    for i, group in enumerate(groups):\n",
    "        dfoi = df[df['group'] == group]\n",
    "        axs.plot(dfoi['step'], dfoi['lin'], label=f'{group}', alpha=0.7)\n",
    "    # for ax in axs:\n",
    "        \n",
    "    axs.plot(dfoi['step'], zero, label='zero-line')\n",
    "    \n",
    "    axs.set_xscale('log')\n",
    "    if ylog:\n",
    "        axs.set_yscale('log')\n",
    "    axs.legend()\n",
    "    axs.set_ylabel('$L_{2,2}$ norm')\n",
    "    axs.set_xlabel('Step size')\n",
    "    fig.suptitle(f\"Non-linearity per step size in direction '{dir}'\")\n",
    "    plt.savefig(f'./plots/{model_type}_{dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model_type: str, dir: str, groups: list, steps: list, test_loader=test_loader):\n",
    "    \"\"\"\n",
    "    calculate linearity per layer given a model and test loader, and plot the results\n",
    "    :param model_name: string of model save filename\n",
    "    :param test_loader: test data loader\n",
    "    \"\"\"\n",
    "    logger.info(f'Starting pipeline for model {model_type} with dir {dir}')\n",
    "    if model_type == 'cnn':\n",
    "        model = load_model('mnist_cnn.pt', 'cnn')\n",
    "        conv = True\n",
    "    elif model_type == 'fc':\n",
    "        model = load_model('mnist_fc.pt', 'fc')\n",
    "        conv = False\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized model type')\n",
    "    input = next(iter(test_loader))\n",
    "    df = forward_runs(input[0], model, groups, dir, steps)\n",
    "    lindf = calc_exp_act(df, conv)\n",
    "    plot(model_type, lindf, dir, True)\n",
    "    return lindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(f'Starting runs')\n",
    "    # groups = [[0,3],[0,4],[1,3],[1,4],[2,4]]\n",
    "    steps = np.logspace(-5,1,6)\n",
    "    # pipeline('fc', 'gradient', groups, steps)\n",
    "    # pipeline('fc', 'axis', groups, steps)\n",
    "    # pipeline('fc', 'random', groups, steps)\n",
    "\n",
    "    groups = [[0,3], [3,6],[6,8]]\n",
    "    df = pipeline('cnn', 'gradient', groups, steps)\n",
    "    pipeline('cnn', 'axis', groups, steps)\n",
    "    pipeline('cnn', 'random', groups, steps)\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
