
@misc{serra_bounding_2018,
	title = {Bounding and {Counting} {Linear} {Regions} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.02114},
	doi = {10.48550/arXiv.1711.02114},
	abstract = {We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation. These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
	month = sep,
	year = {2018},
	note = {arXiv:1711.02114 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Statistics - Machine Learning},
	annote = {Comment: ICML 2018},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/CG8QWJ42/Serra et al. - 2018 - Bounding and Counting Linear Regions of Deep Neura.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/AQYQC2W2/1711.html:text/html},
}

@misc{bouniot_understanding_2023,
	title = {Understanding deep neural networks through the lens of their non-linearity},
	url = {http://arxiv.org/abs/2310.11439},
	doi = {10.48550/arXiv.2310.11439},
	abstract = {The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Bouniot, Quentin and Redko, Ievgen and Mallasto, Anton and Laclau, Charlotte and Arndt, Karol and Struckmeier, Oliver and Heinonen, Markus and Kyrki, Ville and Kaski, Samuel},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11439 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/LPI8FP5U/Bouniot et al. - 2023 - Understanding deep neural networks through the len.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/F5JWVPRI/2310.html:text/html},
}

@incollection{montavon_layer-wise_2019,
	address = {Cham},
	title = {Layer-{Wise} {Relevance} {Propagation}: {An} {Overview}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Layer-{Wise} {Relevance} {Propagation}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_10},
	abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a ‘deep Taylor decomposition’, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
	language = {en},
	urldate = {2024-03-05},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Montavon, Grégoire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and Müller, Klaus-Robert},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_10},
	keywords = {Deep Neural Networks, Deep Taylor Decomposition, Explanations, Layer-wise Relevance Propagation},
	pages = {193--209},
}

@misc{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	doi = {10.48550/arXiv.1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv:1608.04644 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/6NQKNEDW/Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/QXCMV6R5/1608.html:text/html},
}

@misc{qin_adversarial_2019,
	title = {Adversarial {Robustness} through {Local} {Linearization}},
	url = {http://arxiv.org/abs/1907.02610},
	doi = {10.48550/arXiv.1907.02610},
	abstract = {Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47\% adversarial accuracy for ImageNet with l-infinity adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Dvijotham, Krishnamurthy and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet},
	month = oct,
	year = {2019},
	note = {arXiv:1907.02610 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/NPUND5TU/Qin et al. - 2019 - Adversarial Robustness through Local Linearization.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/QISRYT5E/1907.html:text/html},
}

@inproceedings{hu_measuring_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Measuring {Model} {Complexity} of {Neural} {Networks} with {Curve} {Activation} {Functions}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403203},
	doi = {10.1145/3394486.3403203},
	abstract = {It is fundamental to measure model complexity of deep neural networks. A good model complexity measure can help to tackle many challenging problems, such as overfitting detection, model selection, and performance improvement. The existing literature on model complexity mainly focuses on neural networks with piecewise linear activation functions. Model complexity of neural networks with general curve activation functions remains an open problem. To tackle the challenge, in this paper, we first propose linear approximation neural network (LANN for short), a piecewise linear framework to approximate a given deep model with curve activation function. LANN constructs individual piecewise linear approximation for the activation function of each neuron, and minimizes the number of linear regions to satisfy a required approximation degree. Then, we analyze the upper bound of the number of linear regions formed by LANNs, and derive the complexity measure based on the upper bound. To examine the usefulness of the complexity measure, we experimentally explore the training process of neural networks and detect overfitting. Our results demonstrate that the occurrence of overfitting is positively correlated with the increase of model complexity during training. We find that the L1 and L2 regularizations suppress the increase of model complexity. Finally, we propose two approaches to prevent overfitting by directly constraining model complexity, namely neuron pruning and customized L1 regularization.},
	urldate = {2024-03-05},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Xia and Liu, Weiqing and Bian, Jiang and Pei, Jian},
	month = aug,
	year = {2020},
	keywords = {data mining, deep learning, model complexity, model interpretability},
	pages = {1521--1531},
	file = {Submitted Version:/home/teun/Zotero/storage/68W5VVPU/Hu et al. - 2020 - Measuring Model Complexity of Neural Networks with.pdf:application/pdf},
}

@misc{foret_sharpness-aware_2021,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {http://arxiv.org/abs/2010.01412},
	doi = {10.48550/arXiv.2010.01412},
	abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at {\textbackslash}url\{https://github.com/google-research/sam\}.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	month = apr,
	year = {2021},
	note = {arXiv:2010.01412 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/563UWAR9/Foret et al. - 2021 - Sharpness-Aware Minimization for Efficiently Impro.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/MFAJQS98/2010.html:text/html},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/teun/Zotero/storage/MNMEEUIW/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/home/teun/Zotero/storage/AZRI4XJ5/1412.html:text/html},
}
